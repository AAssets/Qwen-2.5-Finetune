import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# ğŸ› ï¸ **1. åŠ è½½æ•°æ®é›†**
def load_sample_data():
    """
    åŠ è½½æ•°æ®é›†å¹¶æå– 1-5 æ¡æ ·æœ¬ã€‚
    """
    data_files = {
        "train": [
            "medical/finetune/train_zh_0.json", 
            "medical/finetune/train_en_1.json"
        ], 
        "validation": [
            "medical/finetune/valid_zh_0.json", 
            "medical/finetune/valid_en_1.json"
        ],
        "test": [
            "medical/finetune/test_zh_0.json", 
            "medical/finetune/test_en_1.json"
        ]
    }
    
    dataset = load_dataset("json", data_files=data_files, split="train")
    
    # æå–å‰5æ¡æ ·æœ¬
    sample_data = dataset.select(range(1))
    print("âœ… å·²æˆåŠŸåŠ è½½ 1 æ¡æ ·æœ¬æ•°æ®ã€‚")
    
    return sample_data


# ğŸ› ï¸ **2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨**
def load_model_and_tokenizer():
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    model_name = "Qwen/Qwen2.5-0.5B"
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,  # ä½¿ç”¨ float16 ç²¾åº¦
        trust_remote_code=True,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right",
        use_fast=False
    )
    
    print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸåŠ è½½ã€‚")
    return model, tokenizer


# ğŸ› ï¸ **3. æ•°æ®é¢„å¤„ç†å‡½æ•°**
def preprocess_function(examples, tokenizer, max_seq_length=1024):
    """
    ä¼˜åŒ–ç‰ˆæ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆä¿®å¤é‡å¤ token å’Œé®è”½é—®é¢˜ï¼‰
    """
    PROMPT_DICT = {
        "prompt_no_input": """<|im_start|>system\n{instruction}<|im_end|>\n<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n""",
        "prompt_input": """<|im_start|>system\n{instruction}<|im_end|>\n<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n""",
    }

    prompts = []
    for i in range(len(examples["instruction"])):
        instruction = examples["instruction"][i]
        input_text = examples["input"][i]
        
        if input_text and input_text.strip():
            prompt = PROMPT_DICT["prompt_input"].format(instruction=instruction, input=input_text)
        else:
            prompt = PROMPT_DICT["prompt_no_input"].format(instruction=instruction)
        
        prompts.append(prompt)

    texts = [prompt + examples["output"][i] for i, prompt in enumerate(prompts)]

    tokenized_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=max_seq_length,
        return_tensors="pt",
        add_special_tokens=False  # é¿å…é‡å¤æ·»åŠ ç‰¹æ®Šæ ‡è®°
    )

    # å¤„ç† labels
    labels = tokenized_inputs["input_ids"].clone()
    for i, prompt in enumerate(prompts):
        prompt_length = len(tokenizer(prompt, truncation=True, max_length=max_seq_length)["input_ids"])
        labels[i][:prompt_length] = -100  # é®è”½è¾“å…¥éƒ¨åˆ†
        labels[i][tokenized_inputs["attention_mask"][i] == 0] = -100  # é®è”½ padding éƒ¨åˆ†

    return {
        "input_ids": tokenized_inputs["input_ids"],
        "attention_mask": tokenized_inputs["attention_mask"],
        "labels": labels
    }



# ğŸš€ **4. æµ‹è¯•ä¸»æµç¨‹**
if __name__ == "__main__":
    print("ğŸ“Š **å¼€å§‹æ•°æ®é¢„å¤„ç†æµ‹è¯•**\n")
    
    # 1. åŠ è½½ç¤ºä¾‹æ•°æ®
    sample_data = load_sample_data()
    
    # 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_and_tokenizer()
    
    # 3. é¢„å¤„ç†ç¤ºä¾‹æ•°æ®
    processed_data = preprocess_function(sample_data, tokenizer, max_seq_length=512)
    
    # 4. æ‰“å°å¤„ç†ç»“æœ
    for i in range(len(sample_data["instruction"])):
        print(f"\nğŸ“ **ç¤ºä¾‹ {i+1}:**")
        print(f"Instruction: {sample_data['instruction'][i]}")
        print(f"Input: {sample_data['input'][i]}")
        print(f"Output: {sample_data['output'][i]}")
        
        print("\nğŸ”— **Processed Text:**")
        decoded_input_ids = tokenizer.decode(processed_data["input_ids"][i], skip_special_tokens=False)
        print(decoded_input_ids)
        
        print("\nğŸ›¡ï¸ **Attention Mask:**")
        print(processed_data["attention_mask"][i])
        
        print("\nğŸ·ï¸ **Labels:**")
        print(processed_data["labels"][i])
    
    print("\nâœ… **æ•°æ®é¢„å¤„ç†æµ‹è¯•å®Œæˆï¼**")
